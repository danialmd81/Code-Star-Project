# Project Introduction

## Overview

In real-world scenarios, data is collected from various sources, each with its own standards and structures. Often, this data is not clean and must be processed before use. Suppose you want to process data on a specific topic collected from different sources with different formats. To achieve meaningful results, you need to clean all the data and convert it into a unified structure and standard.

## What is ETL?

**ETL** stands for **Extract, Transform, Load**. It is the process of copying data from one or more sources to a destination system that represents the data differently.  
[Wikipedia: ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load)

The three main steps in ETL are:

- **Extract**
- **Transform**
- **Load**

### Data Extraction

In this step, raw data is copied from various sources to a staging area. Data sources can be structured or unstructured, such as:

- SQL or NoSQL databases
- Flat files
- Email
- Web pages
- etc.

### Data Transformation

Raw data in the staging area is processed and converted into a structure suitable for analysis. This step may include:

- Filtering
- Cleaning
- Removing duplicates
- Validation
- Calculations, translation, or summarization (e.g., renaming columns, converting currencies or units, editing text columns)
- Removing or encrypting sensitive data
- Converting data structure to the target format (e.g., converting JSON to tables that can be joined)

### Data Loading

Transformed data is moved from the staging area to the target data warehouse. Besides the initial load, this process is often repeated periodically to add new data. In most cases, this is automated.

For more information, check out these resources:

- [IBM: What is ETL?](https://www.ibm.com/cloud/learn/etl)
- [Guru99: ETL Process](https://www.guru99.com/etl-extract-load-process.html)

## Hands-on Practice

To get familiar with ETL, try the following exercise.

### Download

Go to [Knime](https://www.knime.com/) and download the Knime application for your operating system.

### Create a Workflow

After installation, open Knime and create a new workflow as shown below:  
![knime-creating-workflow](./images/KnimeCreatingWrkflow.png)

Name your workflow and download the file [owid-covid-data.csv](https://github.com/owid/covid-19-data/blob/master/public/data/owid-covid-data.csv) for the next step.

### Using the Toolbox

On the left, in the **Nodes** section, you’ll find your toolbox for building workflows.  
For example, add a **CSV Reader** node to the canvas and try reading the downloaded file.

![CSV Reader](./images/CSVReader.png)

> **Tip:**  
> If your file has many rows, go to the node’s **Advanced Settings** and under **Table specification**, disable **Limit data rows scanned**. Then, run the node again.

After a successful run, the node will look like this:

![SuccessReadingCsv](./images/SuccessReadingCsv.png)

### Filter Data

Use toolbox nodes like **Row Filter** to filter data for Iran.

### Data Cleaning

Perform the following cleaning steps:

1. Remove decimal parts from the `new_cases` and `new_deaths` columns.
2. Replace empty values (NaN or null) in the `new_vaccinations` column with zero.
3. Convert the `date` column from string to Date type.

### Aggregation

Calculate the total number of cases per month and store it in a column named `total_month_cases`.

> **Tip:**  
> You may need to modify the date column first to achieve this.

### Join

Upload the [world country latitude and longitude dataset](/datasets/world_country_latitude_and_longitude_values.csv).  
Then, use a **Join** node to add latitude and longitude columns to your COVID statistics dataset.

### Export Output

Finally, export the resulting table as a CSV file and review the results.

Share your workflow with other teams and explore their workflows to see different approaches to the same task.  
To share, go to the **Home** tab, right-click your workflow in **Local space**, and select **Export**.  
