version: "3.8"
services:
  spark-master:
    image: bitnami/spark:3.5.0
    env_file:
      - .env
    environment:
      - SPARK_MODE=master
    # ports:
    #   - "8080:8080" # Web UI
    #   - "7077:7077" # Spark master port
    configs:
      - source: spark-defaults
        target: /opt/bitnami/spark/conf/spark-defaults.conf
      - source: spark-metrics
        target: /opt/bitnami/spark/conf/metrics.properties
    volumes:
      - spark-data:/bitnami
      # TODO #21 find a way for binding ./data in docker swarm cluster
      # - ./data:/data
    networks:
      - etl-network
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      # restart_policy:
      #   condition: on-failure
      # resources:
      #   limits:
      #     cpus: "1"
      #     memory: 1G
      #   reservations:
      #     cpus: "0.5"
      #     memory: 512M

  # Spark worker nodes
  spark-worker:
    image: bitnami/spark:3.5.0
    depends_on:
      - spark-master
    env_file:
      - .env
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    # ports:
    #   - "8081:8081" # Worker Web UI
    volumes:
      - spark-worker-data:/bitnami
      # TODO #21 find a way for binding ./data in docker swarm cluster
      # - ./data:/data
    networks:
      - etl-network
    deploy:
      mode: replicated
      replicas: 2 # Start with 2 workers, adjust based on your needs
      placement:
        constraints:
          - node.role == worker
        preferences:
          - spread: node.id
      # restart_policy:
      #   condition: on-failure
      # resources:
      #   limits:
      #     cpus: "1"
      #     memory: 1G
      #   reservations:
      #     cpus: "0.5"
      #     memory: 512M

  # Spark History Server for job history and logs
  spark-history-server:
    image: bitnami/spark:3.5.0
    command:
      - /opt/bitnami/spark/sbin/start-history-server.sh
    env_file:
      - .env
    environment:
      - SPARK_MODE=master # Using master mode for the history server
      - SPARK_HISTORY_FS_LOG_DIRECTORY=file:///spark-logs
    # ports:
    #   - "18080:18080" # History Server UI
    configs:
      - source: spark-defaults
        target: /opt/bitnami/spark/conf/spark-defaults.conf
    volumes:
      # TODO #21 find a way for binding ./data in docker swarm cluster
      - ~/spark-logs:/spark-logs
    networks:
      - etl-network
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      # resources:
      #   limits:
      #     cpus: "0.25"
      #     memory: 256M
      #   reservations:
      #     cpus: "0.1"
      #     memory: 128M

networks:
  etl-network:
    driver: bridge
    name: etl-network

volumes:
  spark-data:
    driver: local
    name: spark-data
  spark-worker-data:
    driver: local
    name: spark-worker-data

configs:
  spark-defaults:
    file: ./conf/spark-defaults.conf
  spark-metrics:
    file: ./conf/metrics.properties
