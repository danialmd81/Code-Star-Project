version: "3.8"

services:
  # -----------------------------------------------
  # Backend
  # -----------------------------------------------
  team1-backend:
    image: registry:5000/team1/backend:latest
    networks:
      - etl_network
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      restart_policy:
        condition: none

  team2-backend:
    image: registry:5000/team2/backend:latest
    networks:
      - etl_network
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      restart_policy:
        condition: none

  team3-backend:
    image: registry:5000/team3/backend:latest
    networks:
      - etl_network
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      restart_policy:
        condition: none

  # -----------------------------------------------
  # TODO add teams' user to pgadmin from ui
  # Database
  # -----------------------------------------------
  pg-0:
    image: bitnami/postgresql-repmgr:15
    env_file:
      - .env
    environment:
      - REPMGR_NODE_NAME=pg-0
      - REPMGR_NODE_NETWORK_NAME=pg-0
    configs:
      - source: init_script
        target: /docker-entrypoint-initdb.d/01-init.sql
    volumes:
      - database_data:/bitnami/postgresql
    networks:
      - etl_network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=database"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      # resources:
      #   limits:
      #     cpus: "1"
      #     memory: 1G
      #   reservations:
      #     cpus: "0.5"
      #     memory: 512M

  pg-1:
    image: bitnami/postgresql-repmgr:15
    env_file:
      - .env
    environment:
      - REPMGR_NODE_NAME=pg-1
      - REPMGR_NODE_NETWORK_NAME=pg-1
    configs:
      - source: replica_init_script
        target: /docker-entrypoint-initdb.d/01-pg-1-init.sql
    volumes:
      - replica_data:/bitnami/postgresql
    networks:
      - etl_network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=database"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      # resources:
      #   limits:
      #     cpus: "1"
      #     memory: 1G
      #   reservations:
      #     cpus: "0.5"
      #     memory: 512M

  # HAProxy for database
  database:
    image: haproxy:alpine
    entrypoint: ["/bin/sh", "/haproxy-entrypoint.sh"]
    configs:
      - source: ha_proxy_config
        target: /usr/local/etc/haproxy/haproxy.cfg
      - source: haproxy_entrypoint
        target: /haproxy-entrypoint.sh
    # ports:
    # - "5432:5432"
    networks:
      - etl_network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=database"
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.hostname == master-1

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    environment:
      DATA_SOURCE_NAME: "postgresql://postgres_exporter:exporterpass@database:5432/postgres?sslmode=disable"
    # ports:
    #   - "9187:9187"
    networks:
      - etl_network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=database"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker

  # can't have both http and https port ready
  pgadmin:
    image: dpage/pgadmin4:9.5
    env_file:
      - .env
    environment:
      - PGADMIN_ENABLE_TLS=True
      - PGADMIN_CONFIG_SERVER_MODE=True
      - PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED=False
      - PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION=False
      - PGADMIN_CONFIG_UPGRADE_CHECK_ENABLED=False
    secrets:
      - source: ssl_cert
        target: /certs/server.cert
      - source: ssl_key
        target: /certs/server.key
    # ports:
    # - "5050:80"
    # - "5051:443"
    networks:
      - etl_network
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=database"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      # resources:
      #   limits:
      #     cpus: "0.5"
      #     memory: 512M
      #   reservations:
      #     cpus: "0.25"
      #     memory: 256M

  backup:
    image: prodrigestivill/postgres-backup-local:17-alpine
    user: root
    env_file:
      - .env
    environment:
      - POSTGRES_HOST=database
      - POSTGRES_DB=postgres
      - BACKUP_SCHEDULE=0 3 * * *
      - BACKUP_KEEP_DAYS=7
      - BACKUP_KEEP_WEEKS=4
      - BACKUP_KEEP_MONTHS=6
      - BACKUP_DIR=/backups
    volumes:
      - backup_data:/backups
    networks:
      - etl_network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=database"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      # resources:
      #   limits:
      #     cpus: "0.5"
      #     memory: 512M
      #   reservations:
      #     cpus: "0.25"
      #     memory: 256M

  # -----------------------------------------------
  # Frontend
  # -----------------------------------------------
  team1-frontend:
    image: registry:5000/team1/frontend:latest
    networks:
      - etl_network
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      restart_policy:
        condition: none

  team2-frontend:
    image: registry:5000/team2/frontend:latest
    networks:
      - etl_network
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      restart_policy:
        condition: none

  team3-frontend:
    image: registry:5000/team3/frontend:latest
    networks:
      - etl_network
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      restart_policy:
        condition: none

  # -----------------------------------------------
  # Keycloak
  # -----------------------------------------------
  keycloak:
    image: keycloak/keycloak:26.1
    entrypoint:
      [
        "/bin/sh",
        "/wait-for-db.sh",
        "start",
        "--import-realm",
        "--proxy-headers=xforwarded",
      ]
    env_file:
      - .env
    environment:
      - KC_HOSTNAME_STRICT=false
      - KC_HTTP_ENABLED=true
      - KC_HTTPS_CERTIFICATE_FILE=/etc/ssl/certs/fullchain.pem
      - KC_HTTPS_CERTIFICATE_KEY_FILE=/etc/ssl/private/privkey.pem
      - KC_LOG_LEVEL=INFO
      - KC_LOG=file
      - KC_DB_URL_HOST=database
      - KC_DB_URL_PORT=5432
      - KC_DB=postgres
      - KC_DB_URL_DATABASE=keycloak_db
      # JVM settings
      - JAVA_OPTS_APPEND=-Xms1g -Xmx2g -XX:MetaspaceSize=96m -XX:MaxMetaspaceSize=256m -XX:+ParallelRefProcEnabled -XX:+UseStringDeduplication -Djava.net.preferIPv4Stack=true
    configs:
      - source: team1_realm
        target: /opt/keycloak/data/import/team1_realm.json
      - source: team2_realm
        target: /opt/keycloak/data/import/team2_realm.json
      - source: team3_realm
        target: /opt/keycloak/data/import/team3_realm.json
      - source: wait_for_db
        target: /wait-for-db.sh
    secrets:
      - source: ssl_cert
        target: /etc/ssl/certs/fullchain.pem
      - source: ssl_key
        target: /etc/ssl/private/privkey.pem
        # ports:
        # - "8080:8080"
        # - "8443:8443"
    networks:
      - etl_network
    volumes:
      - keycloak_themes:/opt/keycloak/themes
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=keycloak"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      # resources:
      #   limits:
      #     cpus: "1"
      #     memory: 2G
      #   reservations:
      #     cpus: "0.5"
      #     memory: 1G

  # -----------------------------------------------
  # Monitoring
  # -----------------------------------------------
  alertmanager:
    image: prom/alertmanager:v0.26.0
    env_file:
      - .env
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
    configs:
      - source: alertmanager_config
        target: /etc/alertmanager/alertmanager.yml
    # ports:
    # - "9093:9093"
    networks:
      - etl_network
    volumes:
      - alertmanager_data:/alertmanager
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=monitoring"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      # resources:
      #   limits:
      #     cpus: "0.25"
      #     memory: 256M
      #   reservations:
      #     cpus: "0.1"
      #     memory: 128M

  grafana:
    image: grafana/grafana:11.5
    user: "root:root"
    env_file:
      - .env
    environment:
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_DOMAIN=grafana.dani-docker.ir
      # - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel
    configs:
      - source: grafana_dashboards_config
        target: /etc/grafana/provisioning/dashboards/dashboards.yml
      - source: grafana_datasources_config
        target: /etc/grafana/provisioning/datasources/datasources.yml

      - source: grafana_dashboard_cadvisor
        target: /var/lib/grafana/dashboards/cadvisor.json
      - source: grafana_dashboard_docker-swarm
        target: /var/lib/grafana/dashboards/docker-swarm.json
    # ports:
    # - "3000:3000"
    networks:
      - etl_network
    volumes:
      - grafana_data:/var/lib/grafana
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=monitoring"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      # resources:
      #   limits:
      #     cpus: "0.5"
      #     memory: 512M
      #   reservations:
      #     cpus: "0.25"
      #     memory: 256M

  loki:
    image: grafana/loki:3.1.0
    command: -config.file=/etc/loki/local-config.yaml -config.expand-env=true
    configs:
      - source: loki_config
        target: /etc/loki/local-config.yaml
    user: "root:root"
    # ports:
    # - "3100:3100"
    networks:
      - etl_network
    volumes:
      - loki_data:/tmp/loki
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=monitoring"
    deploy:
      mode: replicated
      replicas: 2
      placement:
        constraints:
          - node.role == manager
      # resources:
      #   limits:
      #     cpus: "0.5"
      #     memory: 512M
      #   reservations:
      #     cpus: "0.25"
      #     memory: 256M

  otel-collector:
    image: otel/opentelemetry-collector:0.132.0
    command: ["--config=/etc/otelcol/config.yaml"]
    configs:
      - source: otel_collector_config
        target: /etc/otelcol/config.yaml
    # ports:
    # - "8889:8889" # Prometheus exporter endpoint
    # - "4318:4318" # OTLP HTTP
    networks:
      - etl_network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=monitoring"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      # resources:
      #   limits:
      #     cpus: "0.5"
      #     memory: 512M
      #   reservations:
      #     cpus: "0.25"
      #     memory: 256M

  prometheus:
    image: prom/prometheus:v3.5.0
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=15d"
    configs:
      - source: prometheus_config
        target: /etc/prometheus/prometheus.yml
      # - source: prometheus_rule
      # target: /etc/prometheus/rules/main.yml
    # ports:
    # - "9090:9090"
    networks:
      - etl_network
    volumes:
      - prometheus_data:/prometheus
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=monitoring"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      # resources:
      #   limits:
      #     cpus: "1"
      #     memory: 1G
      #   reservations:
      #     cpus: "0.5"
      #     memory: 512M

  promtail:
    image: grafana/promtail:2.9.3
    command: -config.file=/etc/promtail/promtail.yaml
    configs:
      - source: promtail_config
        target: /etc/promtail/promtail.yaml
    # ports:
    # - "9080:9080"
    networks:
      - etl_network
    volumes:
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=monitoring"
    deploy:
      mode: global
      placement:
        constraints:
          - node.platform.os == linux
      # resources:
      #   limits:
      #     cpus: "0.25"
      #     memory: 256M
      #   reservations:
      #     cpus: "0.1"
      #     memory: 128M

  cadvisor:
    image: bitnami/cadvisor:0.53.0
    user: "root:root"
    command:
      - --docker_only=true
      - --housekeeping_interval=10s
    # ports:
    # - "8080:8080"
    networks:
      - etl_network
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=monitoring"
    deploy:
      mode: global
      placement:
        constraints:
          - node.platform.os == linux
      # resources:
      #   limits:
      #     memory: 256M
      #   reservations:
      #     memory: 128M

  node-exporter:
    image: prom/node-exporter:v1.6.0
    command:
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--path.rootfs=/rootfs"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
    # ports:
    # - "9100:9100"
    networks:
      - etl_network
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=monitoring"
    deploy:
      mode: global
      placement:
        constraints:
          - node.platform.os == linux
      # resources:
      #   limits:
      #     memory: 128M
      #   reservations:
      #     memory: 64M

  # -----------------------------------------------
  # Nginx
  # -----------------------------------------------
  nginx:
    image: nginx:stable-alpine
    entrypoint: ["/bin/sh", "/wait-for-keycloak.sh"]
    env_file:
      - .env
    configs:
      - source: nginx_conf
        target: /etc/nginx/nginx.conf
      - source: database_conf
        target: /etc/nginx/conf.d/database.conf
      - source: frontend_conf
        target: /etc/nginx/conf.d/frontend.conf
      - source: monitoring_conf
        target: /etc/nginx/conf.d/monitoring.conf
      - source: keycloak.conf
        target: /etc/nginx/conf.d/keycloak.conf
      - source: registry_conf
        target: /etc/nginx/conf.d/registry.conf
      - source: spark_conf
        target: /etc/nginx/conf.d/spark.conf
      - source: wait_for_keycloak
        target: /wait-for-keycloak.sh
    secrets:
      - source: ssl_cert
        target: /etc/ssl/certs/fullchain.pem
      - source: ssl_key
        target: /etc/ssl/private/privkey.pem
      - source: registry-auth
        target: /etc/registry/auth/admin
    ports:
      - "80:80" # nginx HTTP
      - "443:443" # nginx HTTPS
    networks:
      - etl_network
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=nginx"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.hostname == master-2
      # restart_policy:
      #   condition: on-failure
      # resources:
      #   limits:
      #     cpus: "0.25"
      #     memory: 256M
      #   reservations:
      #     cpus: "0.1"
      #     memory: 128M

  nginx-exporter:
    image: nginx/nginx-prometheus-exporter:1.4
    command:
      - -nginx.scrape-uri=http://nginx:9113/stub_status
      - -web.listen-address=:9113
    # ports:
    # - "9113:9113"
    networks:
      - etl_network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=nginx"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.hostname == master-2
      # restart_policy:
      #   condition: on-failure
      # resources:
      #   limits:
      #     cpus: "0.25"
      #     memory: 256M
      #   reservations:
      #     cpus: "0.1"
      #     memory: 128M

    # Certificate renewal service

  certbot:
    image: certbot/dns-cloudflare:latest
    env_file:
      - .env
    environment:
      - DOMAIN=dani-docker.ir
      - EMAIL=danialmobinidh81@gmail.com
    configs:
      - source: certbot_renew
        target: /certbot_renew.sh
    secrets:
      - cloudflare_api_token
    volumes:
      - certbot_data:/etc/letsencrypt:rw
      - /var/run/docker.sock:/var/run/docker.sock:rw
    networks:
      - etl_network
    entrypoint: >
      /bin/sh -c '
        trap exit TERM;
        while :; do
          certbot renew \
            --dns-cloudflare \
            --dns-cloudflare-credentials /run/secrets/cloudflare_api_token \
            --deploy-hook "/certbot_renew.sh" \
            --config-dir /etc/letsencrypt \
            --work-dir /tmp \
            --logs-dir /var/log/letsencrypt \
            --quiet;
          sleep 12h & wait $${!};
        done;
      '
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=nginx"
    deploy:
      placement:
        constraints:
          - node.hostname == master-2

  # -----------------------------------------------
  # Registry
  # -----------------------------------------------
  registry:
    image: registry:2
    env_file:
      - .env
    environment:
      - REGISTRY_HTTP_TLS_CERTIFICATE=/etc/ssl/certs/fullchain.pem
      - REGISTRY_HTTP_TLS_KEY=/etc/ssl/private/privkey.pem
      - REGISTRY_AUTH=htpasswd
      - REGISTRY_AUTH_HTPASSWD_PATH=/etc/registry/auth/admin
      - REGISTRY_AUTH_HTPASSWD_REALM="Registry Realm"
      - REGISTRY_STORAGE_DELETE_ENABLED=true
    secrets:
      - source: ssl_cert
        target: /etc/ssl/certs/fullchain.pem
      - source: ssl_key
        target: /etc/ssl/private/privkey.pem
      - source: registry-auth
        target: /etc/registry/auth/admin
    # ports:
    # - "5000:5000"
    networks:
      - etl_network
    volumes:
      - registry_data:/var/lib/registry
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=registry"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker

  registry-ui:
    container_name: registery_ui
    image: joxit/docker-registry-ui:latest
    environment:
      - REGISTRY_TITLE="Dani Registry"
      - REGISTRY_URL=https://registry.dani-docker.ir
    networks:
      - etl_network
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker

  # -----------------------------------------------
  # Spark
  # -----------------------------------------------
  spark-master:
    image: apache/spark:3.5.0
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_PORT=7077
      - SPARK_DAEMON_MEMORY=1g # JVM Options to optimize performance
      - SPARK_DAEMON_JAVA_OPTS=-XX:+UseG1GC -XX:+DisableExplicitGC -XX:+ParallelRefProcEnabled
    configs:
      - source: spark_defaults
        target: /opt/spark/conf/spark-defaults.conf
      - source: spark_metrics
        target: /opt/spark/conf/metrics.properties
    secrets:
      - source: ssl_cert
        target: /opt/spark/conf/cert.pem
      - source: ssl_key
        target: /opt/spark/conf/key.pem
    ports:
      - "7077:7077" # Master Port
      - "6066:6066"
      - "8080:8080" # Master Web UI
      # - "8443:8443" # Master SSL
    networks:
      - etl_network
    volumes:
      - spark_data:/opt/spark/data
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=spark"
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    deploy:
      placement:
        constraints:
          - node.role == worker
    # restart_policy:
    #   condition: on-failure
    # resources:
    #   limits:
    #     cpus: "1"
    #     memory: 1G
    #   reservations:
    #     cpus: "0.5"
    #     memory: 512M

  # Spark worker nodes
  spark-worker:
    image: apache/spark:3.5.0
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_DAEMON_MEMORY=1g # JVM Options to optimize performance
      - SPARK_DAEMON_JAVA_OPTS=-XX:+UseG1GC -XX:+DisableExplicitGC -XX:+ParallelRefProcEnabled
    configs:
      - source: spark_defaults
        target: /opt/spark/conf/spark-defaults.conf
      - source: spark_metrics
        target: /opt/spark/conf/metrics.properties
    secrets:
      - source: ssl_cert
        target: /opt/spark/conf/cert.pem
      - source: ssl_key
        target: /opt/spark/conf/key.pem
    ports:
      - "7080:8080" # Worker Web UI
      # - "7443:8443" # Worker SSL
    networks:
      - etl_network
    volumes:
      - spark_worker_data:/opt/spark/data
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=spark"
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    deploy:
      placement:
        constraints:
          - node.role == worker
        preferences:
          - spread: node.id
    # restart_policy:
    #   condition: on-failure
    # resources:
    #   limits:
    #     cpus: "1"
    #     memory: 1G
    #   reservations:
    #     cpus: "0.5"
    #     memory: 512M

  # Spark History Server for job history and logs
  spark-history-server:
    image: apache/spark:3.5.0
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
    environment:
      - SPARK_MODE=master # Using master mode for the history server
      - SPARK_DAEMON_MEMORY=1g
      - SPARK_DAEMON_JAVA_OPTS=-XX:+UseG1GC -XX:+DisableExplicitGC -XX:+ParallelRefProcEnabled
    configs:
      - source: spark_defaults
        target: /opt/spark/conf/spark-defaults.conf
      - source: spark_metrics
        target: /opt/spark/conf/metrics.properties
    secrets:
      - source: ssl_cert
        target: /opt/spark/conf/cert.pem
      - source: ssl_key
        target: /opt/spark/conf/key.pem
    ports:
      - "6080:8080" # History Server UI
      # - "6443:8443" # History Server SSL
    networks:
      - etl_network
    volumes:
      - spark_history_data:/spark-logs
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "{{.Name}}/{{.ID}}"
        labels: "service=spark"
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == worker
      # resources:
      #   limits:
      #     cpus: "0.25"
      #     memory: 256M
      #   reservations:
      #     cpus: "0.1"
      #     memory: 128M

  # -----------------------------------------------
  # watchtower
  # -----------------------------------------------
  watchtower:
    image: containrrr/watchtower:1.7.1
    command: --interval 60 --cleanup
    env_file:
      - .env
    environment:
      - WATCHTOWER_LABEL_ENABLE=true
      - WATCHTOWER_NOTIFICATION_REPORT=false
      - WATCHTOWER_REGISTRY_AUTH=true
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - etl_network
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager

# -----------------------------------------------
# Networks
# -----------------------------------------------
networks:
  etl_network:
    driver: overlay
    attachable: true
    name: etl_network

# -----------------------------------------------
# Volumes
# -----------------------------------------------
volumes:
  # -----------------------------------------------
  # database
  # -----------------------------------------------
  database_data:
    driver: local
    name: database_data
    driver_opts:
      type: nfs
      o: addr=master-2,rw
      device: ":/srv/docker/volumes/database_data"
  replica_data:
    driver: local
    name: replica_data
    driver_opts:
      type: nfs
      o: addr=master-1,rw
      device: ":/srv/docker/volumes/replica_data"
  pgadmin_data:
    driver: local
    name: pgadmin_data
    driver_opts:
      type: nfs
      o: addr=master-1,rw
      device: ":/srv/docker/volumes/pgadmin_data"
  backup_data:
    driver: local
    name: backup_data
    driver_opts:
      type: nfs
      o: addr=worker1,rw
      device: ":/srv/docker/volumes/backup"

  # -----------------------------------------------
  # Keycloak
  # -----------------------------------------------
  keycloak_themes:
    driver: local
    name: keycloak_themes
    driver_opts:
      type: nfs
      o: addr=master-2,rw
      device: ":/srv/docker/volumes/keycloak_themes"

  # -----------------------------------------------
  # Monitoring
  # -----------------------------------------------
  prometheus_data:
    driver: local
    driver_opts:
      type: nfs
      o: addr=master-2,rw
      device: ":/srv/docker/volumes/prometheus_data"
  grafana_data:
    driver: local
    name: grafana_data
    driver_opts:
      type: nfs
      o: addr=master-1,rw
      device: ":/srv/docker/volumes/grafana_data"
  loki_data:
    driver: local
    name: loki_data
    driver_opts:
      type: nfs
      o: addr=worker1,rw
      device: ":/srv/docker/volumes/loki_data"
  alertmanager_data:
    driver: local
    name: alertmanager_data
    driver_opts:
      type: nfs
      o: addr=worker1,rw
      device: ":/srv/docker/volumes/alertmanager_data"

  # -----------------------------------------------
  # Nginx
  # -----------------------------------------------
  # master-2
  # certbot
  certbot_data:
    driver: local
    name: certbot_data
    driver_opts:
      type: nfs
      o: addr=master-2,rw
      device: ":/srv/docker/volumes/certbot_data"
  # /home/danial/dockerize.ir

  # -----------------------------------------------
  # Registery
  # -----------------------------------------------
  registry_data:
    driver: local
    name: registry_data
    driver_opts:
      type: nfs
      o: addr=master-1,rw
      device: ":/srv/docker/volumes/registry_data"

  # -----------------------------------------------
  # Spark
  # -----------------------------------------------
  spark_data:
    driver: local
    name: spark_data
    driver_opts:
      type: nfs
      o: addr=master-2,rw
      device: ":/srv/docker/volumes/spark_data"
  spark_worker_data:
    driver: local
    name: spark_worker_data
    driver_opts:
      type: nfs
      o: addr=worker1,rw
      device: ":/srv/docker/volumes/spark_worker_data"
  spark_history_data:
    driver: local
    name: spark_history_data
    driver_opts:
      type: nfs
      o: addr=master-2,rw
      device: ":/srv/docker/volumes/spark_history_data"

# -----------------------------------------------
# Configs
# -----------------------------------------------
configs:
  # -----------------------------------------------
  # database
  # -----------------------------------------------
  init_script:
    file: ./services/database/init/01-init.sql
  replica_init_script:
    file: ./services/database/init/01-init.sql
  ha_proxy_config:
    file: ./services/database/haproxy/haproxy.cfg
  haproxy_entrypoint:
    file: ./services/database/haproxy/haproxy-entrypoint.sh
  # -----------------------------------------------
  # Keycloak
  # -----------------------------------------------
  team1_realm:
    file: ./services/keycloak/realm-config/team1-realm.json
  team2_realm:
    file: ./services/keycloak/realm-config/team2-realm.json
  team3_realm:
    file: ./services/keycloak/realm-config/team3-realm.json
  wait_for_db:
    file: ./services/keycloak/wait-for-db.sh

  # -----------------------------------------------
  # Monitoring
  # -----------------------------------------------
  alertmanager_config:
    file: ./services/monitoring/alertmanager/alertmanager.yml
  grafana_dashboards_config:
    file: ./services/monitoring/grafana/provisioning/dashboards/dashboards.yml
  grafana_datasources_config:
    file: ./services/monitoring/grafana/provisioning/datasources/datasources.yml

  grafana_dashboard_cadvisor:
    file: ./services/monitoring/grafana/dashboards/cadvisor.json
  grafana_dashboard_docker-swarm:
    file: ./services/monitoring/grafana/dashboards/docker-swarm.json

  loki_config:
    file: ./services/monitoring/loki/local-config.yaml

  otel_collector_config:
    file: ./services/monitoring/otel-collector/config.yaml

  prometheus_config:
    file: ./services/monitoring/prometheus/prometheus.yml
  # prometheus_rule:
  # file: ./services/monitoring/prometheus/rules/alerts.yml

  promtail_config:
    file: ./services/monitoring/promtail/promtail.yaml

  # -----------------------------------------------
  # Nginx
  # -----------------------------------------------
  nginx_conf:
    file: ./services/nginx/nginx.conf
  database_conf:
    file: ./services/nginx/conf.d/database.conf
  frontend_conf:
    file: ./services/nginx/conf.d/frontend.conf
  monitoring_conf:
    file: ./services/nginx/conf.d/monitoring.conf
  keycloak.conf:
    file: ./services/nginx/conf.d/keycloak.conf
  registry_conf:
    file: ./services/nginx/conf.d/registry.conf
  spark_conf:
    file: ./services/nginx/conf.d/spark.conf
  wait_for_keycloak:
    file: ./services/nginx/wait-for-keycloak.sh
  certbot_renew:
    file: ./services/nginx/cert-renew.sh

  # -----------------------------------------------
  # Spark
  # -----------------------------------------------
  spark_defaults:
    file: ./services/spark/conf/spark-defaults.conf
  spark_metrics:
    file: ./services/spark/conf/metrics.properties
# -----------------------------------------------
# Secrets
# -----------------------------------------------
secrets:
  ssl_cert:
    external: true
    # file: ./secrets/fullchain.pem
  ssl_key:
    external: true
    # file: ./secrets/privkey.pem

  cloudflare_api_token:
    file: ./secrets/cloudflare.ini

  registry-auth:
    file: ./services/registry/auth/admin
